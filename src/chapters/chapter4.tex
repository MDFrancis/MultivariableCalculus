
We want to develop the calculus necessary to discuss functions
of many variables.   We shall start with functions
$f(x,y)$ of two independent
variables and functions $f(x,y,z)$ of three independent
variables.   However, in general, we need to consider functions
$f(x_1, x_2, \dots, x_n)$ of any number 
of independent variables.   We shall use the notation $\R^n$ as
before to stand for the set of all $n$-tuples $(x_1,x_2,\dots,x_n)$
with real entries $x_i$.  For $n = 2$, we shall identify 
$\R^2$ with the plane and, for $n = 3$, we shall identify $\R^3$
with space.   We shall use the old fashioned term \emph{locus}
to denote the set of all points satisfying some equation or condition.
\index{locus}
  

\section{Graphing in Many Variables}

We shall encounter equations involving two, three, or more variables.
As you know, an equation of the form
\[
     f(x,y) = C
\]
may be viewed as defining a curve in the plane.  For example,
$ax + by = c$ has plane locus a line, while $x^2 + y^2 = R^2$
has plane locus a circle of radius $R$ centered at the origin.
Similarly, an equation involving three variables
\[
   f(x,y,z) = C
\]
may be thought of as defining a \emph{surface} in space.
Thus, we saw previously that the locus in $\R^3$ of a linear equation
\[
   ax + by + cz = d
\]
(where not all $a, b$, and $c$ are zero) is a plane.   If we
use more complicated equations, we get more complicated surfaces.

\begin{example}
	The equation
	\[
	   x^2 + y^2 + z^2 = R^2
	\]
	may be rewritten $\norm{\vec r} = \sqrt{x^2 + y^2 + z^2} = R$, so it
	asserts that the point with position vector $\vec r$ is at distance
	$R$ from the origin.  Hence, the locus of all such points is a
	\emph{sphere} of radius $R$ centered at the origin.
\end{example}

\begin{example}
	Consider the locus of the equation
	\[
	  x^2 + 2x + y^2 - 4y + z^2 = 20.
	\]
	This is also a sphere, but one not centered at the origin.  To
	see this, \emph{complete the squares} for the terms involving
	$x$ and  $y$.
	\begin{align*}
	    x^2 + 2x + 1 + y^2 -4y + 4 + z^2 &= 10 + 1 + 4 = 25 \\
	    (x + 1)^2 + (y - 2)^2 + z^2 &= 5^2.
	\end{align*}
	This asserts that the point with position vector $\vec r = ( x, y, z )$
	is 5 units from the point $(-1, 2, 0)$, i.e., it lies on a sphere
	of radius 5 centered at $(-1, 2, 0)$.
\end{example}

\begin{example}
	Consider the locus of the equation $z = x^2 + y^2$ (which could
	also be written $x^2 + y^2 - z = 0$.)  To see what this looks like,
	we consider its intersection with various planes.   Its intersection
	with the $y,z$-plane is obtained by setting $x = 0$ to get
	$z = y^2$.  This is a parabola in the $y,z$-plane.  
	Similarly, its intersection with the $x,z$-plane is the parabola
	given by $z = x^2$.   To fill in the picture,  consider
	intersections with planes parallel to the $x,y$-plane.  Any
	such plane has equation $z = h$, so the intersection has equation
	$x^2 + y^2 = h = (\sqrt h)^2$, which you should recognize as a circle
	of radius $\sqrt h$, at least if $h > 0$.  Note that the circle is
	centered at $(0,0,h)$ on the $z$-axis since it lies in the plane $z = h$.
	If $z = h = 0$, the circle reduces to a single point, and for 
	$z = h < 0$, there is no locus.
	The surface is ``bowl'' shaped.  It is called a \emph{circular
	paraboloid}.

	XXX Figure
\end{example}

Graphing a surface in $\R^3$ by sketching its traces on various
planes is a useful strategy.  In order to be good at it, you need
to know the basics of plane analytic geometry so you can recognize
the resulting curves.  In particular, you should be familiar with
the elementary facts concerning \emph{conic sections}, i.e.,
ellipses, hyperbolas, and parabolas.   Edwards and Penney, 3rd Edition,
Chapter 10 is a good reference for this material.

\begin{example}
	Consider the locus in space of $\displaystyle{\frac{x^2}4 +
	\frac{y^2} 9} = 1$.   Its intersection with a plane $z = h$
	parallel to the $x,y$-plane is an ellipse centered on the $z$-axis
	and with semi-minor and semi-major axes  2 and 3.   The surface
	is a \emph{cylinder} perpendicular to the $x,y$-plane with
	elliptical cross sections.
	Note that the locus \emph{in space} is not just the ellipse
	in the $x,y$-plane with the same equation.
	
	XXX Figure
\end{example}

\begin{example}
	Consider the locus in space of the equation
	$z = \dfrac 1{x^2 + y^2}$.   Its intersection with the plane $z = h$
	(for $h > 0$) is the circle with equation
	$ x^2 + y^2 = 1/h = (\sqrt{1/h})^2$.   The surface does not intersect
	the $x,y$-plane itself ($z = 0$) nor any plane below the $x,y$-plane.
	It intersection with the $x,z$-plane ($y = 0$) is the curve
	$z = 1/x^2$ which is asymptotic to the $x$-axis
	and to the positive $z$-axis.  Similarly, for its intersection
	with the $y,z$-plane   The surface flattens out and approaches the
	$x,y$-plane as $r = \sqrt{x^2 + y^2} \to \infty$.  It approaches the
	positive $z$-axis as $r \to 0$.
\end{example}

\begin{example}
	Consider the locus in space of the equation $yz = 1$.   Its intersection
	with a plane parallel to the $y,z$-plane ($x = d$) is a hyperbola
	asymptotic to the $y$ and $z$ axes.  The surface is perpendicular
	to the $y,z$-plane.  Such a surface is also called a \emph{cylinder}
	although it doesn't close upon itself as the elliptical cylinder considered
	above.
\end{example}

\begin{example}
	Consider the locus of the equation $x^2 + z^2 = y^2 - 1$.  For
	each plane parallel to the $x,z$-plane ($y = c$), the intersection
	is a circle $x^2 + z^2 = c^2 - 1 = (\sqrt{c^2 - 1})^2$ centered
	on the $y$-axis, at least of $c^2 > 1$.  For $y = c = \pm 1$,
	the locus is a point, and for $-1 < y = c < 1$, the locus is
	empty.   In addition, the intersection of the surface with the
	$x,y$-plane ($z = 0$) is the hyperbola with equation $x^2 - y^2
	 = -1$, and similarly for its intersection with the $y,z$-plane.
	The surface comes in two pieces which open up as ``bowls'' centered
	on the positive and negative $y$-axes.   The surface is called
	a \emph{hyperboloid of 2 sheets}.    

	XXX Figure
\end{example}

\subsection{Graphing Functions}
For a scalar function $f$ of one independent variable, 
the \emph{graph of the function} is the set of
\index{graph of a function}
all points in $\R^2$  of the form  $(x, f(x))$
for $x$ in the domain of the function.   (The domain of a function
is the set of values of the independent variable for which the
\index{domain of a function}
function is defined.)  In other words, it is the locus of the
equation $y = f(x)$.  It is generally a curve in the plane.

We can define a similar notion for a scalar function $f$ of two
independent variables.  The graph is the
set of points in $\R^3$  of the form $(x, y, f(x,y))$
for $(x,y)$ a point in the domain of the function.  In other words,
it is the locus of the equation $z = f(x,y)$, and it is generally
a surface in space.
The graph of a function is often useful in understanding the
function.

We have already encountered several examples of graphs of functions.
For example, the locus of $z = x^2 + y^2$ is the graph of the
function $f$ defined by $f(x,y) = x^2 +y^2$.   Similarly, the locus
of $z = 1/(x^2 + y^2)$ is the graph of the function $f$ defined by
$f(x,y) = 1/(x^2 + y^2)$ for $(x,y) \not= (0,0)$. 
  Note that in the first case there need
be no restriction on the domain of the function, but in the second
case $(0,0)$ was omitted.  

In some of the other examples, the locus
of the equation cannot be considered the graph of a function.
For example, the equation $x^2 + y^2 + z^2 = R^2$ cannot be solved
uniquely for $z$ in terms of $(x,y)$.  Indeed, we have
$z = \pm\sqrt{R^2 - x^2 - y^2}$, so that two possible functions
suggest themselves.  $z = f_1(x,y) = \sqrt{R^2 - x^2 - y^2}$ 
defines a function with graph
the \emph{top hemisphere} of the sphere, while $z = f_2(x,y)
= - \sqrt{R^2 - x^2 - y^2}$ yields the lower hemisphere.  (Note that
for either of the functions the relevant domain is the set of
points on or inside the circle $x^2 + y^2 = R^2$.  For points outside
that circle, the expression inside the square root is negative, so,
since we are only talking about functions assuming real values,
such points must be excluded.)

\begin{example}
	Let $f(x,y) = xy$ for all $(x,y)$ in $\R^2$.    The graph is the
	locus of the the equation $z = xy$.  We can sketch it by considering
	traces on various planes.  Its intersection with a plane parallel
	to the $x,y$-plane ($z =$ constant) is a hyperbola asymptotic to
	lines parallel to the $x$ and $y$ axes.   For $z > 0$, the hyperbola
	is in the first and third quadrants of the plane, but for
	$z < 0$ it is in the second and fourth quadrants.  For $z = 0$,
	the equation is $xy = 0$ with locus consisting of the $x$-axis
	($y = 0$) and the $y$-axis ($x = 0$).   Thus, the graph intersects
	the $x,y$-plane in two straight lines.  Th surface is generally
	shaped like an ``infinite saddle''.  It is called a {\it hyperbolic
	paraboloid}.  It is clear where the term ``hyperbolic'' comes from.
	Can you see any parabolas?  (Hint: Try planes perpendicular to
	the $x,y$-plane with equations of the form  $y = mx$.)

	XXX Figure
\end{example}

\begin{example}
	 Let $f(x,y) = x/y$ for $y \not= 0$.  Thus, the domain of this
	function consists of all points $(x,y)$ not on the $x$-axis
	($y = 0$).
	The trace in the plane $y = c, c \not= 0$ is the line $z = (1/c)x$
	with slope $1/c$.   Similarly, the trace in the plane $z = c, c \not=0$
	is the line $y = (1/c)x$.  Finally, the trace in the plane $x = c$,
	is the hyperbola $z = c/x$.  Even with this information you will have
	some trouble visualizing the graph.   However, the equation
	$z = x/y$ can be rewritten $yz = x$.   By permuting the variables,
	you should see that the locus of $yz = x$
	 is similar to the  saddle shaped surface
	we just described, but oriented differently in space.  However,
	 the saddle
	is not quite the graph of the function since it  contains the 
	 $z$-axis ($y = x = 0$) but the graph of the function does not.
	In general, the graph of a function, since it consists of points
	of the form $(x,y,f(x,y))$, cannot contain points with the same
	values for $x$ and $y$ but different values for $z$.  In other words,
	any line parallel to the $z$-axis can intersect such a graph at most
	once. 
\end{example}

Sketching graphs of functions, or more generally loci of equations
in $x, y$, and $z$, is not easy.   One approach drawn from the
study of topography is to interpret the equation $z = f(x,y)$ as
giving the \emph{elevation} of the surface, viewed as a hilly
terrain, above a reference plane.  (Negative elevation
 $f(x,y)$ is
interpreted to mean that the surface dips below the reference plane.)
For each possible elevation $c$,
 the intersection of the plane $z = c$ with the graph
yields a curve $f(x,y) = c$.  This curve is called a \emph{level curve}, 
and we draw a 2-dimensional map of the graph by sketching
\index{level curve}
the level curves and labeling each by the appropriate elevation
$c$.  Of course, there are generally infinitely many level curves
since there are infinitely many possible values of $z$, but we
select some subset to help us understand the topography of the
surface.

XXX Figure

\begin{example}
	The level curves of the surface $z = xy$ have equations
	$xy = c$ for various $c$.  They form a 
	a family of hyperbolas, each with two branches.  For $c > 0$,
	these hyperbolas fill the first and third quadrants, and for
	$c < 0$ they fill the second and fourth quadrants.  For $c  = 0$
	the $x$ and $y$ axes together constitute the level ``curve''.
	See the diagram.

	You can see that the region around the origin $(0,0)$ is like a
	``mountain pass'' with the topography rising in the first and
	third quadrants and dropping off in the second and fourth quadrants.
	In general a point where the graph behaves this way is called
	a \emph{saddle point}.   Saddle points indicate the added complexity
	which can arise when one goes from functions of one variable to
	functions of two or more variables.  At such points, the function
	can be considered as having a maximum  or
	 a minimum depending on where you
	look.
\end{example}

\subsection{Quadric Surfaces}
One important class of surfaces are those defined by quad\-rat\-ic
equations.  These are analogs in three dimensions of
conics in two dimensions.  They are called \emph{quadric surfaces}.  
We describe here \emph{some} of the possibilities.
You can verify the pictures by using the methods described above.
\index{quadric surface}

Consider first equations of the form
\[
   \pm \frac{x^2}{a^2} 
   \pm \frac{y^2}{b^2} 
   \pm \frac{z^2}{c^2} = 1
\]

If all the signs are positive, the surface is called an {\it 
ellipsoid}.
\index{ellipsoid}
Planes perpendicular to one of the coordinate axes intersect it
in ellipses (if they intersect at all).   However, at the extremes
these ellipses degenerate into the points $(\pm a, 0, 0),
(0,\pm b, 0)$, and $(0,0,\pm c)$.

XXX Figure

If exactly one of the signs are negative, the surface is called a
\emph{hyperboloid of one sheet}.   It is centered on one axis
\index{hyperboloid of one sheet}
(the one associated to the negative coefficient in the equation)
and it opens up in both positive and negative directions along
that axis.  Its intersection with planes perpendicular to that
axis are ellipses.   Its intersections with planes perpendicular to
the other axes are hyperbolas.


If exactly two of the signs are negative, the surface is called
a \emph{hyperboloid of two sheets}.   It is centered on one
\index{hyperboloid of two sheets}
axis (associated to the positive coefficient).  For example,
suppose the equation is
\[
  -\frac{x^2}{a^2}
  +\frac{y^2}{b^2}
  -\frac{z^2}{c^2} = 1.
\]
For $y < -b$ or $y > b$, the graph intersects a plane perpendicular
to the $y$-axis in an ellipse.  For $y = \pm b$, the intersection
is the point $(0,\pm b, 0)$.  (These two points are called vertices
of the surface.)  For $-b < y < b$, there is no intersection with
a plane perpendicular to the $y$-axis.


The above surfaces are called \emph{central quadrics}.
\index{central quadric}
Note that for the hyperboloids, with equations in standard form as
above, the number of sheets is the same as
the number of minus signs.

Consider next equations of the form
\[
   z = \pm \frac{x^2}{a^2} \pm \frac{y^2}{b^2}
\]
(or similar equations obtained by permuting $x, y$ and $z$.)

If both signs are the same, the surface is called an {\it elliptic
paraboloid}.   If both signs are positive, it is centered on the
\index{elliptic paraboloid}
\index{paraboloid, elliptic}
positive $z$-axis and its intersections with planes perpendicular to
the positive $z$-axis are a family of similar ellipses which increase
in size as $z$ increases.   If both signs are negative, the situation
is similar, but the surface lies below the $x,y$ plane.


If the signs are different, the surface is called a {\it
hyperbolic paraboloid}.  Its intersection with planes perpendicular
to the $z$-axis are hyperbolas asymptotic to the lines in
those planes parallel to the lines $x/a = \pm y/b$.   Its intersection
with the $x,y$-plane is just those two lines.  The surface has
a saddle point at the origin.
\index{hyperbolic paraboloid}
\index{paraboloid, hyperbolic}
\index{saddle}


The locus of the equation $z = cxy, c\not= 0$ is also a hyperbolic
paraboloid, but rotated so it intersects the $x,y$-plane in the
$x$ and $y$ axes. 

XXX Figure

Finally, we should note that many so called ``degenerate conics''
are loci of quadratic equations.   For example, consider
\[
    \frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = 0
\]
which may be solved to obtain
\[
   z = \pm c\sqrt{\frac{x^2}{a^2} + \frac{y^2}{b^2}}. 
\]

The locus is a double cone with elliptical cross sections and
vertex at the origin.  

\subsection{Generalizations}
In general, we will want to study functions of any number of independent
variables.   For example, we may define the graph of a scalar valued
function $f$ of three independent variables to be the set of all points
in $\R^4$ of the form $(x,y,z,f(x,y,z))$.   Such an object
should be considered a three dimensional subset of $\R^4$, and it is
certainly not easy to visualize.  It is more useful to consider the
analogues of level curves for such functions.  Namely, for each
possible value $c$ attained by the function, we may consider the
locus in $\R^3$ of the equation $f(x,y,z) = c$.  This is generally
a surface called a \emph{level surface} for the function. 
\index{level surface}

\begin{example}
For $f(x, y, z) = x^2 + y^2 + z^2$, the level surfaces are
concentric spheres centered at the origin if $c > 0$.   For
$c = 0$ the level `surface' is not really a surface at all; it
just consists of the point at the origin.   (What if $c < 0$?) 

For $f(x,y,z) = x^2 + y^2 - z^2$, the level surfaces are either
hyperboloids of one sheet if $c > 0$ or hyperboloids of two sheets
if $c < 0$.  (What if $c = 0$?)
\end{example}

For functions of four or more variables, geometric interpretations
are even harder to come by.  If $f(x_1,x_2,\dots,x_n)$ denotes
a function of $n$ variables, the locus in $\R^n$  of the
equation $f(x_1,x_2,\dots,x_n) = c$ is called a level set,
but one doesn't ordinarily try to visualize it geometrically.  

Instead of talking about many independent variables, it is useful
to think instead of a single independent variable which is a 
\emph{vector}, i.e., an element of $\R^n$ for some $n$.   In the
case  $n = 2, 3$, we usually write $\vec r = ( x,y )$ or
$\vec r = ( x,y,z )$ so $f(x,y)$ or $f(x,y,z)$ would be written 
simply $f(\vec r)$.  If $n > 3$, then one often denotes the
variables $x_1, x_2, \dots, x_n$ and denotes the vector
(i.e., element of $\R^n$) by $\vec x = (x_1,x_2, \dots, x_n)$.
Then $f(x_1,x_2, \dots, x_n)$ becomes simply  $f(\vec x)$.
The case of a function of a single real variable can be subsumed
in this formalism by allowing the case $n =1$.  That is,
we consider a scalar $x$ to be just a vector of dimension 1,
i.e., an element of $\R^1$.

When we talked about kinematics, we considered  \emph{vector valued}
functions $\vec r(t)$ of a single independent variable.    Thus we see
that it makes sense to consider in general functions of a vector
variable which can also assume vector values.   We indicate this
by the notation $f:\R^n \to \R^m$.   That shall mean that the domain
of the function $f$ is a subset of $\R^n$ while the set of values is
a subset of $\R^m$.    Thus, $n = m = 1$ would yield a scalar
function of one variable, $n = 2, m = 1$ a scalar function of two
variables, and $n = 1, m = 3$ a vector valued function of one scalar
variable.   We shall have occasion to consider several other special
cases in detail.

There is one slightly non-standard aspect to the above notation.
In ordinary usage in mathematics,
``$f:\R^n \to \R^m$'' means that $\R^n$ is the entire domain of the
function $f$, whereas we are taking it to mean that the domain is some
subset.    We do this mostly to save writing since usually the
domain will be almost all of $\R^n$  or at least some significant
chunk of it.  What we want to make clear by the notation is the
dimensionality of both the independent and dependent variables.

\begin{exercises}
You are encouraged to make use of the available computer software
(e.g., Maple, Mathematica, etc.) to help you picture the graphs
in the following problems.


\begin{enumerate}
	\item   State the largest possible domain for the function 
	\begin{enumerate}
		\item $f(x,y) =  e^{ x^2-y^2}$ 
		\item $f(x,y) = \ln (y^2-x^2-2)$ 
		\item $f(x,y) = \dfrac {x^2-y^2}{x-y}$ 
		\item $f(x,y,z) = \dfrac {1}{xyz}$ 
		\item $f(x,y,z) = \dfrac {1}{\sqrt {z^2-x^2-y^2}}$ 
	\end{enumerate}

	\item   Describe the graph of the function described by 
	\begin{enumerate}
		\item $f(x,y) = 5$ 
		\item $f(x,y) = 2x - y$ 
		\item $f(x,y) = 1-x^2-y^2$ 
		\item $f(x,y) = 4 - \sqrt {x^2+y^2}$ 
		\item $f(x,y) = \sqrt {24 - 4x^2 - 6y^2}$ 
	\end{enumerate}

	\item   Sketch selected level curves for the functions given by
	\begin{enumerate}
		\item $f(x,y) = x+y$
		\item $f(x,y) = x^2 + 9y^2$
		\item $f(x,y) = x - y^2$
		\item $f(x,y) = x - y^3$
		\item $f(x,y) = x^2 + y^2 + 4x + 2y +9$
	\end{enumerate}

	\item   Describe selected level surfaces for the functions
	given by 
	\begin{enumerate}
		\item $f(x,y,z) = x^2 + y^2 - z$ 
		\item $f(x,y,z) = x^2 + y^2 + z^2 + 2x - 2y + 4z$ 
		\item $f(x,y,z) = z^2 - x^2 - y^2$ 
	\end{enumerate}

	\item   Describe the quadric surfaces which are loci in $\R^3$
	of the following equations. 
	\begin{enumerate}
		\item $x^2 + y^2 = 16$ 
		\item $z^2 = 49x^2 + y^2$ 
		\item $z= 25 - x^2 - y^2$ 
		\item $x=4y^2-z^2$ 
		\item $4x^2 + y^2 + 9z^2 = 36$ 
		\item $x^2 + y^2 - 4z^2 = 4$ 
		\item $9x^2 + 4y^2 -z^2 = 36$ 
		\item $9x^2 - 4y^2 - z^2 = 36$
	\end{enumerate}

	\item   Describe the traces of the following functions in the given planes
	\begin{enumerate}
		\item $z=xy$, in horizontal planes  $z = c$ 
		\item $z=x^2+9y^2$ in vertical planes $x = c$ or $y = c$ 
		\item $z=x^2+9y^2$ in horizontal planes  $z = c$
	\end{enumerate}

	\item   Describe the intersection of the cone $x^2 + y^2 = z^2$ with
	the plane $z = x + 1$.  

	\item   Let $\vec f:\R^2 \to \R^2$.  
	\begin{enumerate}
		\item Try to invent a definition of `graph' for such a function.   For
		    what $n$ would it be a subset of $\R^n$? 
		\item Try to invent a definition of `level set' for such a function.
		    For what $n$ would it be a subset of $\R^n$?
	\end{enumerate}

\end{enumerate}
\end{exercises}

\section{Rates of Change and the Directional Derivative}
	
For a function $f:\R\to\R$ of one variable we have the idea of
the \emph{rate of change} of $f$.  We might ask, ``rate of change with
respect to what?''  For a function of one variable this question has an obvious
answer---the rate of change with respect to the only thing that varies!
If $g:\R^n\to\R$ is a multi-variable function, the question of ``what is the rate of change
of $g$?'' has a less obvious answer.  After all, there are multiple variables and
infinitely many directions in the domain.

Consider the function $g:\R^2\to\R$ where $g(x,y)=(x-2)^2+y^2$.  Let $\vec u$ be a unit
vector.  Now, starting at $(0,0)$, we might ask approximately how much $g$ changes
when we move $\alpha$ units away from $(0,0)$ in the direction $\vec u$.

XXX Figure

Since $\vec u$ is a unit vector, the displacement in the domain is $\alpha\vec u$, and so the
exact change in $g$ from $\vec 0$ to $\alpha\vec u$ is $g(\alpha\vec u)-g(\vec 0)$.  The approximate
\emph{rate} of change\index{rate of change} is then $\frac{g(\alpha\vec u)-g(\vec 0)}{\alpha}$.  Taking
a limit as $\alpha\to 0$ will give us an instantaneous rate of change---one worth name.

\begin{definition}[Rate of change with respect to distance]
	For a function $f:\R^n\to\R$, the \emph{rate of change with respect to distance} of $f$
	in the direction $\vec v$ at the point $\vec a$ is
	\[
		\lim_{h\to 0^+}\frac{f(\vec a+h\vec v)-f(\vec a)}{\norm{h\vec v}}.
	\]
\end{definition}

We must divide by $\norm{h\vec v}$ in the definition of rate of change with respect to distance
because $\norm{h\vec v}$ is exactly how far we've displaced.  Of course, if we were forward-thinking
enough to pick $\vec v$ to be a unit vector, then $\norm{h\vec v}=\abs{h}$.

We can think of the rate of change with respect to distance in another way.  Suppose that $f:\R^n\to \R$
and that $\vec p:\R\to\R^n$ is the arc-length parameterization of the with direction
vector $\vec v$ and where $\vec p(0)=\vec a$.  Then, $f\circ \vec p:\R\to\R$ is a single-variable
function and the rate of change of $\vec f$ at $\vec a$ in the direction $\vec v$ is just $(f\circ\vec p)'(0)$.
Think for a moment about why this is.

The rate of change of a function with respect to distance is, in some sense, the most natural
notion of ``rate of change'' for a multivariable function.  (If you're handed a function, what
would you measure the rate of change against if not distance?)  However, it turns out not
to be the most useful notion of rate of change.  For that, we introduce the 
\emph{directional derivative}\index{directional derivative}.

\begin{definition}[Directional Derivative]
	For a function $f:\R^n\to\R$, the \emph{directional derivative} of $f$ at the point $\vec a$
	in the direction $\vec v$ is
	\[
		D_{\vec a}f(\vec v) = \lim_{h\to0}\frac{f(\vec a+h\vec v)-f(\vec a)}{h}.
	\]
\end{definition}

The directional derivative $D_{\vec a}f(\vec v)$ corresponds to the rate of change of $f$ at the point $\vec a$
if you moved in the direction of $\vec v$ with speed $\norm{\vec v}$.  This seems 
like a less intuitive notion than rate of change with respect to distance, but its virtues will
soon become clear\footnote{ For starters, $D_{\vec a}f$ is \emph{linear}.  Namely, if $f$ is differentiable,
then $D_{\vec a}f(\vec u+\vec v)=D_{\vec a}f(\vec u)+D_{\vec a}f(\vec v)$.  The same cannot
be said for rate of change with respect to distance.}.

\begin{example}
	Let $f:\R^2\to\R$ be given by $f(x,y)=(x-2)^2-y^2$.  Compute $D_{\vec 0}f(1,2)$.

	Since we have no further theory to lean on, we must use the definition of the directional
	derivative directly.

	\begin{align*}
		D_{\vec 0}f(1,2) &= \lim_{h\to0}\frac{f(\vec 0+h(1,2))-f(\vec 0)}{h}
		=\lim_{h\to0}\frac{((h-2)^2-(2h)^2)-4}{h}\\
		&=\lim_{h\to0}\frac{(-3h^2-4h+4)-4}{h}=-4.
	\end{align*}
\end{example}

The directional derivative can be used to approximate a function.  Recall that if $g:\R\to\R$ is
a differentiable function of one variable, then
\[
	g(a+h)\approx f(a) + hg'(a).
\]
Similarly, for $f:\R^2\to\R$ we have\footnote{ Assuming $f$ has directional derivatives, of course.}
\begin{equation}
	\label{EQDIRDERIVAPPROX}
	f(\vec a+h\vec v)\approx f(\vec a)+hD_{\vec a}f(\vec v).
\end{equation}

The similarity in these two formulas is one reason
why directional derivatives are more useful than rates of change with respect to distance\footnote{
	If we think about vectors in $\R^1$, the number $1$ is actually a vector.  Therefore,
	for $g:\R\to\R$, we have $g'(1)=D_{a}g(1)$.  Try writing out the limit expression.  It's the same
	as the usual definition of derivative!
	}.

\begin{exercise}
	Create the analogous equation to Equation \eqref{EQDIRDERIVAPPROX} but approximate
	$f(\vec a+h\vec v)$ using the rate of change with respect to distance instead of the
	directional derivative.
\end{exercise}


\subsection{Partial Derivatives}
There are some particularly common directional derivatives.  Namely, those in the directions
of the standard basis.  Let $f:\R^2\to\R$ and let $\vec a=(a_x,a_y)$.  Now,
\begin{align*}
	D_{\vec a}f(\xhat) &= \lim_{h\to0}\frac{f(\vec a+h\xhat)-f(\vec a)}{h}
	=\lim_{h\to0}\frac{f(a_x+h, a_y)-f(a_x,a_y)}{h}.
\end{align*}

This limit looks quite similar to a one-dimensional derivative.  However, $f$ does not
have a one-dimensional domain.  Accordingly we call $D_{\vec a}f(\xhat)$ a 
\emph{partial derivative}\index{partial derivative} and we have a special notation for it.

\begin{definition}
	For a function $f:\R^n\to\R$, let $x_i$ denote the $i$th input variable.
	The \emph{partial derivative} of $f$ at $\vec a=(a_1,\ldots,a_n)$ with respect to $x_i$ is 
	notated $\frac{\partial f}{\partial x_i}(\vec a)$ and is defined to be
	\[
		\frac{\partial f}{\partial x_i}(\vec a) = 
		\lim_{h\to0}\frac{f(a_1,\ldots,a_{i-1},a_i+h,a_{i+1},\ldots,a_n)-f(\vec a)}{h}.
	\]
\end{definition}
Sometimes we'll write $\frac{\partial f}{\partial x_i}$, omitting the point where the partial
derivative is taken, if we want to save space, or if we want to view a partial derivative as a function
of where it is taken.  For functions whose domains are $\R^2$ or $\R^3$, we often use $x,y,z$ 
instead of $x_1,x_2,x_3$ to denote
the first, second, or third variable.  It is not important how you write your variable as long
as it is clear to you and your reader which variable is changing.

\subsection{Tangent Vectors to Surfaces}
When we had a curve $\mathcal S$ parameterized by $\vec p:\R\to\mathcal S$,
we could find tangent vectors to $\mathcal S$ by computing the velocity vectors of $\vec p$.
In particular, $\vec p\,'(t_0)$ is a tangent vector to $\mathcal S$ at the point $\vec p(y_0)$.
We will find tangent vectors to surfaces in a similar way---by parameterizing a path on the surface.

Suppose $f:\R^2\to \R$ is a function and let $\mathcal S$ be the surface given by $z=f(x,y)$.  Formally,
\[
	\mathcal S=\Set*{\mat{x\\y\\z}\given z=f(x,y)}.
\]
Since $\mathcal S$ is the graph of a function whose domain is $\R^2$ we have a natural parameterization
of $\mathcal S$ coming from $f$.  That is,
\[
	\vec p:\R^2\to\mathcal S\qquad\text{where}\qquad \vec p(x,y)=\matc{x\\y\\f(x,y)}
\]
is a parameterization of $\mathcal S$.

Let $\vec a=(a_x,a_y,f(a_x,a_y))\in\mathcal S$ and consider the function $\vec r_x:\R\to\R^2$
given by $\vec r_x(t) = (a_x+t,a_y)$.  The function $\vec r_x$ starts at the point $(a_x,a_y)$
in the $xy$-plane and moves parallel to the $x$-axis with speed one.  Since we have parameterized
$\mathcal S$ by the $xy$-plane, by composing $\vec p$ and $\vec r_x$, we will get a parameterization
of a curve on $\mathcal S$.  That is, $\vec c=\vec p\circ \vec r_x$ parameterizes the curve
on the surface $\mathcal S$ that when viewed from above looks like the line $y=a_y$.

XXX Figure

Further, $\vec c(0)=(a_x,a_y,f(a_x,a_y))=\vec a$.  Thus, if we want a tangent
vector to $\mathcal S$ at $\vec a$, all we need to do is compute $\vec c\,'(0)$.

Computing,
\[
	\vec c\,'(0)=(\vec p\circ \vec r_x)'(0)=
	\left(\matc{a_x+t\\a_y\\f(a_x+t,a_y)}'\text{ at $t=0$ } \right)
	= \matc{1\\0\\\frac{\partial f}{\partial x}(a_x,a_y)}.
\]

Of course, this was just one choice of curve along $\mathcal S$.  We could
have just as easily used $\vec p\circ \vec r_y$ where $\vec r_y(t)=(a_x,a_y+t)$
as a parameterization of a curve.  We could have parameterized paths
that weren't parallel to the $x$ or $y$ axes.  For instance
$\vec p\circ \vec r_{d}$ where $\vec r_{d}(t) = (a_x+td_x, a_y+td_y)$
for non-zero constants $d_x,d_y$.

\begin{example}
	Find at least three tangent vectors at the point $\vec a=(1,2,12)$
	to the surface $\mathcal S$ defined by $z=f(x,y)$ where $f(x,y)=(x-3)^2+y^3$.

	Let $\vec r_a(t) = (1+t,2)$, $\vec r_b(t)=(1,2+t)$, and $\vec r_c(t)=
	(1+t,2+t)$.  Further, let $\vec p:\R^2\to\R^3$ be defined by $\vec p(x,y)=
	(x,y,f(x,y))$.  Since $\vec p$ is a parameterization of $\mathcal S$,
	$\vec p\circ \vec r_a$, $\vec p\circ \vec r_b$, 
	and $\vec p\circ \vec r_c$ all parameterize paths in $\mathcal S$.  Further,
	\[\vec p\circ \vec r_a(0)=\vec p\circ \vec r_b(0)=\vec p\circ \vec r_c(0)=\vec a,\]
	so the velocity vectors at $t=0$ of these parameterizations will give 
	tangent vectors to $\mathcal S$ at $\vec a$.
	
	Computing, we see
	\[
	(\vec p\circ \vec r_a)'(0)
	= \matc{1\\0\\\frac{\partial f}{\partial x}(1,2)} = \mat{1\\0\\-4},
	\]
	\[
	(\vec p\circ \vec r_b)'(0)
	= \matc{1\\0\\\frac{\partial f}{\partial y}(1,2)} = \mat{0\\1\\12},
	\]
	and
	\[
	(\vec p\circ \vec r_c)'(0)
	= \matc{1\\1\\ D_{(1,2)}f(1,1)} = \mat{1\\1\\8}
	\]
	are all tangent vectors to $\mathcal S$ at $\vec a$.
\end{example}



\begin{exercises}
\end{exercises}

\section{Tangent Planes and Differentiability}

Suppose $f:\R^2\to\R$ is a function and consider the surface $\mathcal S\subseteq \R^3$
given by the equation $z=f(x,y)$.  If $f$ is a ``nice'' function, like a polynomial, 
the surface $\mathcal S$ will be smooth.  Visually, at each point on $\mathcal S$ we
could imagine a tangent plane---the analog of a tangent line to a curve.

XXX Figure

More formally, fix a point $\vec a\in\mathcal S$ and consider the set $\mathcal P$
of all tangent vectors to $\mathcal S$ at $\vec a$.  If $\mathcal S$ is a smooth
surface, these tangent vectors will lie in a single plane.  In other words,
$\mathcal P$ is a plane, and in this case we call it the 
\emph{tangent plane}\index{tangent plane}
to $\mathcal S$ at $\vec a$.  (Fantastically, if $\mathcal P$ is a plane,
it can be fully described by the point $\vec a$ and \emph{two} non-parallel
tangent vectors.  We'll make use of this idea later.)

XXX Figure

Tangent planes to surfaces, like tangent lines to curves, have
the property that they are a very good approximation to a surface
at their point of tangency.  We will leverage this idea to 
define what it means to be differentiable for a multivariable function.

\subsection{Directional Derivatives and Planes}

A plane $\mathcal P$ is a \emph{set} and is distinct from any equations
or parameterizations.  However, if $\mathcal P\subseteq \R^3$ is not a vertical
plane (i.e., one for which $\zhat$ is a direction vector), then it can be represented
uniquely by an equation of the form $z=\alpha x+\beta y+c$.  In other words,
a non-vertical plane in $\R^3$ is the graph of a function $f:\R^2\to\R$
which takes the form $f(x,y)=\alpha x+\beta y+c$.  Functions of this form
are known as \emph{affine functions}\index{affine function}, and they
often show up in the context of \emph{linear approximations}\index{linear approximations}\footnote{
	In many contexts in calculus, the term ``linear approximation'' is used
	where ``affine approximation'' would be more accurate.  Sometimes, though,
	it's easiest not to fight the tide of established culture.}.

\begin{definition}[Affine Function]
	A function $f:\R^n\to\R$ is an \emph{affine function} if it can
	be expressed as
	\[
		f(x_1,x_2,\ldots,x_n) = \alpha_1x_1+\alpha_2x_2+\cdots+\alpha_nx_n +c
	\]
	for some $\alpha_1,\ldots,\alpha_n,c\in\R$.  Equivalently, $f:\R^n\to\R$ is
	an affine function if it can be expressed as
	\[
		f(\vec x)=\vec \alpha \cdot \vec x+c
	\]
	for some $\vec\alpha\in\R^n$ and $c\in \R$.
\end{definition}


Directional derivatives of affine functions take a particularly nice form.
Let $f:\R^2\to\R$ be an affine function.
That is, $f(x,y)=\alpha x+\beta y+c$ for some $\alpha,\beta,c\in\R$. 
Let $\vec a=(a_x,a_y)$ and $\vec v=(v_x,v_y)$.  Now,

\begin{align*}
	D_{\vec a}f(\vec v) &= \lim_{h\to0}\frac{f(\vec a+h\vec v)-f(\vec a)}{h}
	=\lim_{h\to0}\frac{f(a_x+hv_x, a_y+hv_y)-f(a_x,a_y)}{h}\\
	&=\lim_{h\to0}\frac{\alpha(a_x+hv_x)+\beta( a_y+hv_y)+c-(\alpha a_x+\beta a_y+c)}{h}\\
	&=\lim_{h\to0}\frac{\alpha hv_x+\beta hv_y}{h} = \alpha v_x+\beta v_y\\
	&=\mat{\alpha\\\beta}\cdot \vec v.
\end{align*}

We have written a formula for the directional derivative of $f$ in an arbitrary direction!  There's more.
\[
	\alpha = \mat{\alpha\\\beta}\cdot \xhat = \frac{\partial f}{\partial x}(\vec a)
	\qquad\text{ and }\qquad
	\beta = \mat{\alpha\\\beta}\cdot \yhat = \frac{\partial f}{\partial y}(\vec a)
\]
and so
\[
	D_{\vec a}f(\vec v) = \mat{\tfrac{\partial f}{\partial x}(\vec a)\\\tfrac{\partial f}{\partial y}(\vec a)}
	\cdot \vec v.
\]
The vector $\mat{\tfrac{\partial f}{\partial x}(\vec a)\\\tfrac{\partial f}{\partial y}(\vec a)}$ comes up often.
It is called the \emph{gradient}\index{gradient}.

\begin{definition}[Gradient]
	For a function $f:\R^n\to\R$, the \emph{gradient} of $f$ at the point $a$ is
	written $\nabla f(\vec a)$ and is defined to be the vector
	\[
		\nabla f(\vec a) = \Big(\tfrac{\partial f}{\partial x_1}(\vec a),
		\ldots, \tfrac{\partial f}{\partial x_n}(\vec a) \Big).
	\]
\end{definition}

Using the notation of \emph{gradient}, 
we conclude (using the same $f(x,y)=\alpha x+\beta y+c$ from earlier) that
\begin{equation}
	\label{EQDIRDERIVGRAD}
	D_{\vec a}f(\vec v) = \nabla f(\vec a)\cdot \vec v.
\end{equation}
All directional derivatives of affine functions can be expressed as a dot product with
the gradient.  So far, we have only proven this for
affine function $f:\R^2\to\R$, but it is true for affine functions
$g:\R^n\to\R$ as well.  This fact is important enough to write down
as a theorem.

\begin{theorem}
	\label{THMAFFINEDERIV}
	If $f:\R^n\to\R$ is an affine function, then
	$
		D_{\vec a}f(\vec v) = \nabla f(\vec a)\cdot \vec v
	$
	for all $\vec a,\vec v\in\R^n$.
\end{theorem}

Note that at the moment, the gradient is just a bag of derivatives---it
doesn't yet have meaning for us beyond a convenient notational trick.

So far the rule in Equation \eqref{EQDIRDERIVGRAD} only applies to affine functions.
It would be nice if it worked for other functions.  So, in true mathematical
fashion, we will define a class of functions based on whether or not Equation
\eqref{EQDIRDERIVGRAD} applies\footnote{ For technical reasons we won't do exactly this.
If we considered the set of functions for which Equation \eqref{EQDIRDERIVGRAD}
holds at every point, we'd never have a reasonable chain rule for multivariable
functions.  Instead we'll use the slightly stronger notation of \emph{linear approximations}.}.

\subsection{Linear Approximations}

Recall from single variable calculus, if $f:\R\to\R$ was a differentiable function,
a \emph{linear approximation} of $f$ at the point $a\in\R$ was a function $L:\R\to\R$
whose graph is the tangent line to $y=f(x)$ at the point $(a,f(a))$.

XXX Figure

Further, $L$ always takes the form $L(x)=\alpha(x-a)+c$.  In fact, if $f'$ exists,
we can be more
specific:  \[L(x)=f'(a)(x-a)+f(a).\]
The approximation $L$ is quite good around the point $a$, so we have
\[
	f(a+\Delta x)\approx L(a+\Delta x) = f'(a)\Delta x+f(a)
\]
if $\Delta x$ is small.  It's a little unclear though what ``$\approx$''
exactly means.  In your single-variable calculus class you may or may not
have made this precise.  Visually, the line given by $y=L(x)=f'(a)(x-a)+f(a)$
is distinguished from all other lines passing through the point $(a,f(a))$ because
it is \emph{tangent}.  Analytically, the linear approximation $L(x)=f'(a)(x-a)+f(a)$
is distinguished from all other linear approximations to $f$ at $a$ because it satisfies
the property
\begin{equation}
	\label{EQONEVARLINAPPROX}
	\lim_{\Delta x\to 0} \frac{f(a+\Delta x)-L(a+\Delta x)}{\Delta x} = 0.
\end{equation}

\begin{exercise}
	Let $L(x)=\alpha(x-a)+c$ and let $f:\R\to\R$ be a differentiable function.
	Show that if $L$ satisfies Equation \eqref{EQONEVARLINAPPROX} then it
	must be the case that $c=f(a)$ and $\alpha=f'(a)$.
\end{exercise}

Equation \eqref{EQONEVARLINAPPROX} encapsulates such a strong idea that
it could in fact be used (and sometimes is used) to define the derivative
of a single-variable function.  We will use the multi-dimensional
analog of Equation \eqref{EQONEVARLINAPPROX} to define differentiability.

\begin{definition}[Differentiable]
	The function $f:\R^n\to \R$ is 
	\emph{differentiable}\index{differentiable}
	at $\vec a=(a_1,\ldots,a_n)$ if there exists
	an affine function $p(x_1,\ldots, x_n)
	= c+\sum \alpha_i (x_i-a_i) = c+\vec \alpha \cdot(\vec x-\vec a)$ so that
	\[
		f(\vec a)=p(\vec a)\qquad\text{and}\qquad\lim_{\vec u\to0} \frac{f(\vec a + \vec u)-p(\vec a + \vec u)}{\norm{\vec u}} = 0.
	\]
\end{definition}

The definition of differentiability can be interpreted in two equivalent
ways.  One is that a function is differentiable at a point $\vec a$ if there exists a ``good''
linear approximation to the function at $\vec a$ (where ``good'' is defined
to mean that it satisfies the limit).  Alternatively, one could think
about the graph of a function as a surface.  A function $f$ is then differentiable
at $\vec a$ if there exists a tangent plane to the surface given by the graph
of $f$ at the point $(\vec a, f(\vec a))$.  Being able to switch back between these
two perspectives will aid your intuition.

However, there's a big problem with this definition of differentiability.  It
involves a multivariable limit\footnote{ If you already know how to evaluate
multi-dimensional limits, this isn't much of a problem.}.  Multivariable limits
are much more subtle than single-variable limits, and they are covered in the next
section.  But before we do that, we will further explore the consequences of 
differentiability.

\subsection{Consequences of Differentiability}

Recall, directional derivatives of affine functions can be expressed
as dot products with the gradient vector.  This is true in general
for differentiable functions.

\begin{theorem}
	\label{THMDIFFERENTIABLE}
	If $f:\R^n\to\R$ is differentiable at the point $\vec a$, then
	\[
		D_{\vec a}f(\vec v)=\nabla f(\vec a)\cdot \vec v
	\]
	for all $\vec v\in\R^n$.
\end{theorem}
\begin{proof}
	Let $f:\R^n\to\R$ be a function that is differentiable at
	the point $\vec a$, and let $L:\R^n\to\R$ be the unique affine function
	such that $f(\vec a)=L(\vec a)$ and 
	\begin{equation}
		\label{EQGRADWORKS}
		\lim_{\vec u\to\vec 0}
		\frac{f(\vec a+\vec u)-L(a+\vec u)}{\norm{\vec u}}=0.
	\end{equation}
	By Theorem \ref{THMAFFINEDERIV}, $D_{\vec a}L(\vec v)=\nabla L(\vec a)\cdot \vec v$
	for all $\vec v$.

	Now, let us recall a fact about limits. If $h,g$ are functions and
	$\lim_{x\to a}h(x)$ exists and $\lim_{x\to a}(h(x)+g(x))$ exists, then
	we must have $\lim_{x\to a}g(x)$ exists and
	\[
		\lim_{x\to a} \Big(h(x)+g(x)\Big)=
		\lim_{x\to a} h(x)+\lim_{x\to a}g(x).
	\]
	
	Putting it all together, Equation \eqref{EQGRADWORKS} implies
	for any $\vec v$,
	\[
		0=0\norm{\vec v}=\norm{\vec v}\lim_{h\to 0} 
		\frac{f(\vec a+h\vec v)-L(a+h\vec v)}{\norm{h\vec v}}
		=
		\lim_{h\to 0} 
		\frac{f(\vec a+h\vec v)-L(a+h\vec v)}{h}
	\]
	exists.  In particular, this shows that $D_{\vec a}(f-L)(\vec v)=0$.
	Applying our fact about limits, since $D_{\vec a}L(\vec v)$ exists
	and $D_{\vec a}(f-L)(\vec v)$ exists, we must have that $D_{\vec a}f(\vec v)$
	exists and that
	\[
		D_{\vec a}f(\vec v) - D_{\vec a}L(\vec v)=0.
	\]
	In other words,
	\begin{equation}
		\label{EQGRADWORKS2}
		D_{\vec a}f(\vec v) = D_{\vec a}L(\vec v) = \nabla L(\vec a)\cdot \vec v.
	\end{equation}

	To finish off the proof, notice that
	\[
		\frac{\partial L}{\partial x}(\vec a)=
		D_{\vec a}L(\xhat) = D_{\vec a}f(\xhat)=\frac{\partial f}{\partial x}(\vec a).
	\]
	Repeating this argument with $\yhat$,
	$\zhat$, etc\mbox{.} shows that the partial derivatives of $f$ at $\vec a$
	must equal the partial derivatives of $L$ at $\vec a$.  Therefore,
	\[
		\nabla L(\vec a)=\nabla f(\vec a).
	\]
	Substituting $\nabla L(\vec a)$ with $\nabla f(\vec a)$ in Equation
	\eqref{EQGRADWORKS2} completes the proof.
\end{proof}

Theorem \ref{THMDIFFERENTIABLE} shows that if a function is differentiable,
directional derivatives can be computed with dot products instead of limits!

\begin{example}
	Find the directional derivative of the function $f:\R^3\to\R$
	given by $f(x,y,z)=x^2-z^3$ at the point $\vec a=(1,1,1)$ in the direction
	$\vec v=(1,2,3)$.

	Since $f$ is a polynomial it is differentiable.  Computing,
	\[
		\nabla f(1,1,1) = \mat{2\\0\\-3}
	\]
	and so
	\[
		D_{\vec a}f(\vec v) = \nabla f(\vec a)\cdot \vec v=-7.
	\]
\end{example}

\subsection{Geometric Interpretation of the Gradient}

Suppose $f:\R^n\to \R$ is a differentiable function and $\vec u$ is
a unit vector.  We know that
\[
	D_{\vec a}f(\vec u) = \nabla f(\vec a)\cdot \vec u = 
	\norm{\nabla f(\vec a)}\norm{\vec u}\cos\theta=\norm{\nabla f(\vec a)}\cos\theta,
\]
where $\theta$ is the angle between $\vec u$ and $\nabla f(\vec a)$.
Since $\cos \theta \leq 1$,
we see that $D_{\vec a}f(\vec u)$ is maximized when $\theta=0$.  In other words,
\emph{the largest directional derivative 
of $f$ occurs in the direction of the gradient of $f$}.  Thus, the gradient may be interpreted
as pointing in the direction of greatest change.

XXX Figure

Continuing this further, if $D_{\vec a}f(\vec u)=0$ then one of two things 
must occur.  Either $\nabla f(\vec a)=\vec 0$ or $\theta=90^{\circ}$.  Therefore,
if $\nabla f(\vec a)\neq \vec 0$, the direction in which the function doesn't
change at all is orthogonal to the gradient.

We have another word for paths on which a function doesn't change--level curves.
If we plot level curves for a function, the gradient of the function must always be
orthogonal to the level curves.

XXX Figure


\begin{exercises}
\end{exercises}

\section{Multidimensional Limits and Continuity}

Most users of mathematics don't worry about things that might
go wrong with the functions they use to represent physical quantities.
They tend to assume that  functions  are differentiable when derivatives
are called for  (except possibly for a finite set of isolated points),
and they assume all functions which need to be integrated are continuous
so the integrals will exist.   For much of the period during which
Calculus was developed (during the 17th and 18th centuries), mathematicians
also did not bother themselves with such matters.   Unfortunately,
during the 19th century, mathematicians discovered that general
functions could behave in  unexpected and subtle ways, so they began to 
devote much more time to careful formulation of definitions and careful
proofs in analysis.   This is an aspect of mathematics which is
covered in courses in real analysis, so we won't devote much time
to it in this course.  (You may have noticed that we didn't worry
about the existence of derivatives in our discussion of
velocity and acceleration.)  However, 
for functions of several variables,
lack of rigor can be more troublesome than in
the one variable case,  so we briefly devote some attention to such
questions.
 In this section, we shall discuss the concepts
of \emph{limit} and \emph{continuity} for functions $f:\R^2 \to \R$.
The big step, it turns out, is going from one independent 
variable to two.
Once you understand that, going to three or more
independent variables introduces
few additional difficulties.

Let  $\vec r_0 = ( x_0, y_0 )$ be (the position vector of) a point
in the domain of the function $f$.   We want to define the
concept to be expressed symbolically
\[
    \lim_{\vec r \to \vec r_0} f(\vec r) = L \qquad\text{or}\qquad 
   \lim_{(x,y)\to (x_0,y_0)} f(x,y) = L.
\]
\index{limit of a function of several variables}
We start with two examples which illustrate the concept and some
differences from the single variable case.

\begin{example}
	\label{EXCONTLIMIT}
	Let $f(x,y) = x^2 + 2y^2$, and consider the
	nature of the graph of $f$  near the point $(1,2)$.  As
	we saw in the previous section, the graph is an elliptic paraboloid,
	the locus of $z = x^2 + 2y^2$.
	In particular, the surface is quite smooth, and if $(x,y)$ is
	a point in the domain \emph{close to} $(1,2)$, then $f(x,y)$
	will be very close to the value of the function there, $f(1,2) =
	1^2 + 2(2^2) = 9$.   Thus, it makes sense to assert that
	\[
	 \lim_{(x,y)\to (1,2)} x^2 + 2y^2 = 9.
	\]

	XXX Figure
\end{example}

In Example \ref{EXCONTLIMIT}, the limit was determined simply by evaluating the
function at the desired point.  You may remember that in the single
variable case, you cannot always do that.
For example, putting $x = 0$ in  $\sin x/x$ yields the meaningless
expression $0/0$, but $\lim_{x \to 0} \sin x/x$ is known to be 1.
Usually, it requires some ingenuity to find such examples in the single
variable case, but the next example shows that fairly simple formulas
can lead to unexpected difficulties for functions of two or more
variables.

\begin{example}
	\label{EXDISCONT}
	Let
	\[
	   f(x,y) = \frac{x^2 - y^2}{x^2 + y^2}\qquad\text{for } (x,y) \not= (0,0).
	\]
	 What does the graph of this function look like in the vicinity
	of the point $(0,0)$?   (Since, $(0,0)$ is not in the domain of
	the function, it does not make sense  to talk about
	$f(0,0)$, but we can still seek a `limit'.)  The easiest way to
	answer this question is to switch to polar coordinates.   Using
	$x = r\cos\theta, y = r\sin\theta$, we find
	\[
	f(\vec r) = f(x,y) = \frac{r^2\cos^2\theta - r^2\sin^2\theta}
	{r^2\cos^2\theta + r^2\sin^2\theta} = \cos^2\theta - \sin^2\theta
	 = \cos 2\theta.
	\]
	Thus, $f(\vec r) = f(x,y)$
	is independent of the polar coordinate $r$ and depends only
	on $\theta$.  As $r = \norm{\vec r} \to 0$ with $\theta$ fixed, $f(\vec r)$
	is constant, and equal to $\cos 2\theta$, so, if  it `approaches'
	a limit, that limit would have to be $\cos 2\theta$.  
	Unfortunately, $\cos 2\theta$ varies between $-1$ and $1$, so it
	does not make sense to say $f(\vec r)$ has a limit as $\vec r \to \vec 0$.
	You can get some idea of what the graph looks like by studying the
	level curves which are pictured in the diagram.  For each value
	of $\theta$, the function is constant, so the level curves consist
	of rays emanating from the origin, as indicated.  On any such
	ray, the graph is at some constant height $z$ with $z$ taking on
	\emph{every value} between $-1$ and $+1$.
\end{example}

In general, the statement
\[
  \lim_{\vec r \to \vec r_0} f(\vec r) = L
\]
will be taken to mean that $f(\vec r)$ is \emph{close to} $L$ whenever
$\vec r$ is \emph{close to } $\vec r_0$.   To make this completely
rigorous, we will give an ``$\varepsilon$-$\delta$ definition'' of limit.
 
\begin{definition}[Limit]
	Let $f:\R^n\to \R$ be a function.  For a point $\vec a$,
	we say \emph{the limit of $f$ as $\vec x$ approaches
	$\vec a$ is $L\in\R$}, written
	\[
		\lim_{\vec x\to \vec a}f(\vec x)=L,
	\]
	if for all numbers $\varepsilon>0$ there exists a number
	$\delta >0$ so that whenever
	\[
		\norm{\vec x-\vec a}<\delta\qquad\text{ we have }\qquad\abs{f(\vec x)-L}<\varepsilon.
	\]
\end{definition}

In this statement, $\norm{\vec x - \vec a} < \delta$ asserts that the distance
from $\vec x$ to $\vec a$ is less than $\delta$.  Since $\delta$ is
thought of as small, the inequality  makes precise the meaning of
`$\vec x$ is close to $\vec a$'.   Similarly, $\abs{f(\vec x) - L} < 
\epsilon$ catches the meaning of ``$f(\vec x)$ is close to 
$L$.''  Note that we never consider the case $\vec x = \vec a$,
so the value of $f(\vec r_0)$
 is not relevant in checking the limit as $\vec x \to \vec a$.
(It is not even necessary that $f(\vec r)$  be well defined at
$\vec x = \vec a$.)

Limits for functions of several variables behave formally much
the same as limits for functions of one variable.  Thus, you
may calculate the limit of a sum by taking the sum of the
limits, and similarly for products and quotients (except that
for quotients the limit of the denominator should not be zero).
The understanding you gained of these matters in the single
variable case should be an adequate guide to what to expect
for several variables.  If you never really understood all this
before, we won't enlighten you much here.  You will have to
wait for a course in real analysis for real understanding.

\subsection{Continuity}
In Example \ref{EXCONTLIMIT}, the limit was determined simply by evaluating the
function at the point.  This is certainly not always possible
because the value of the function may be irrelevant or there
may be no 
meaningful 
way to 
attach a value.   Functions
for which it is always possible to find the limit this way
 are called
\emph{continuous}.  (This is the same notion as for functions
\index{continuous function of several variables}
of a single scalar variable).  More precisely, we say that
$f$ is continuous at a point $\vec r_0$ if the point is in its domain (i.e.,
$f(\vec r_0)$ is defined) and
\[
  \lim_{\vec r \to \vec r_0} f(\vec r) = f(\vec r_0).
\]
Points at which this fails  are called \emph{discontinuities}
or sometimes \emph{singularities}.   (The latter term is also sometimes
\index{discontinuity}
reserved for less serious kinds of mathematical pathology.)  It
sometimes happens, that a function $f$ has a well defined limit $L$ at
a point $\vec r_0$ which does not happen to be in the domain of the function,
i.e., $f(\vec r_0)$ is not defined.  (In the single variable case,
$\sin x/ x$ at $x = 0$ is a good example.)  Then we can extend the
domain of the function to include the point $\vec r_0$ by defining
$f(\vec r_0) = L$.   Thus the original function had a discontinuity,
but it can be eliminated simply by extending the definition of the
function.  In this case, the discontinuity is called
\emph{removable}.   As Example \ref{EXDISCONT} shows, there are functions
with  discontinuities
which cannot be defined away no matter what you try.  
 
  A function without discontinuities is called continuous.
Continuous functions have graphs which look reasonably 
smooth.  They don't have big holes or sudden jumps,
 but as we shall see later, they can still look pretty
bizarre.   Usually, just knowing that a function is continuous
won't be enough to make it a good candidate to represent a
physical quantity.  We shall also want to be able to take
derivatives and do the usual things one does in differential
calculus, but as you might expect, this is somewhat more involved
than it is in the single variable case. 


\begin{exercises}
\end{exercises}

\section{Optimization}

\begin{exercises}
\end{exercises}

